version: '3.7'

services:
  db:
    restart: always
    image: postgres
    container_name: mlflow_db1
    expose:
      - "${PG_PORT}"
    networks:
      - backend
    environment:
      - POSTGRES_USER=${PG_USER}
      - POSTGRES_PASSWORD=${PG_PASSWORD}
      - POSTGRES_DATABASE=${PG_DATABASE}
    volumes:
      - ./monitoring/db_data:/var/lib/postgres/data
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "${PG_PORT}", "-U", "${PG_USER}"]
      interval: 5s
      timeout: 5s
      retries: 3

  s3:
    restart: always
    image: minio/minio
    container_name: mlflow_minio1
    volumes:
      - ./minio_data:/data
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    networks:
      - frontend
      - backend
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_ADDRESS=${MINIO_ADDRESS}
      - MINIO_PORT=${MINIO_PORT}
      - MINIO_STORAGE_USE_HTTPS=${MINIO_STORAGE_USE_HTTPS}
      - MINIO_CONSOLE_ADDRESS=${MINIO_CONSOLE_ADDRESS}
    command: server /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  create_buckets:
    image: minio/mc
    container_name: mlflow_create_buckets1
    depends_on:
      - s3
    networks:
      - backend
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc config host add s3 http://s3:${MINIO_PORT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_ACCESS_KEY} --api S3v4;
      [[ ! -z "`/usr/bin/mc ls s3 | grep challenge`" ]] || /usr/bin/mc mb s3/${MLFLOW_BUCKET_NAME};
      /usr/bin/mc policy download s3/${MLFLOW_BUCKET_NAME};
      exit 0;
      '

  mlflow_server:
    restart: always
    build: ./mlflow
    image: mlflow_server
    container_name: mlflow_server1
    depends_on:
      - db
    ports:
      - "${MLFLOW_PORT}:5000"
    networks:
      - frontend
      - backend
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=http://s3:${MINIO_PORT}
      - MLFLOW_S3_IGNORE_TLS=true
    command: >
      mlflow server
      --backend-store-uri postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT}/${PG_DATABASE}
      --host 0.0.0.0
      --serve-artifacts
      --artifacts-destination s3://${MLFLOW_BUCKET_NAME}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${MLFLOW_PORT}/"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - /tmp/mlopsdb:/tmp/mlopsdb
      - /tmp/mlopsartifacts:/tmp/mlopsartifacts
      - /tmp/store:/tmp/store

  prefect_server:
    restart: always
    build:
      context: ./model_orchestration_and_tracking
      dockerfile: Dockerfile-prefect
    logging:
      driver: none
    image: prefect_server
    container_name: prefect_server
    ports:
      - 4200:4200
      - 8081:8081
    env_file:
      - ./model_orchestration_and_tracking/.env
    networks:
      - backend
    environment:
      PREFECT__SERVER__HOST: http://172.17.0.1
      PREFECT__SERVER__PORT: 4200
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
    #   SQLALCHEMY_DATABASE_URI="postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT}/${PG_DATABASE}"
    volumes:
      - /tmp/mlopsdb:/tmp/mlopsdb
      - /tmp/mlopsartifacts:/tmp/mlopsartifacts
      - /tmp/store:/tmp/store
    
  prefect_agent:
    restart: always
    depends_on:
      - prefect_server
      - mlflow_server
    build:
      context: ./model_orchestration_and_tracking
      dockerfile: Dockerfile-prefect-agent
    image: prefect_agent1
    container_name: prefect_agent1
    environment:
      # MONGODB_ADDRESS: "mongodb://127.0.0.1:27017"
      PREFECT__SERVER__HOST: http://172.17.0.1
      PREFECT__SERVER__PORT: 4200
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
      REPORTS_FOLDER: /tmp/mlreports
      REPORT_TIME_WINDOW_MINUTES: 180
      EVIDENTLY_TIME_WIDTH_MINS: 720
      MLFLOW_TRACKING_URI: "http://localhost:${MLFLOW_PORT}"
      # GOOGLE_APPLICATION_CREDENTIALS: /secrets/mlops-credit-risk-secret.json
      # SQLALCHEMY_DATABASE_URI: "postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT}/${PG_DATABASE}"
    volumes:
      - /tmp/mlopsdb:/tmp/mlopsdb
      - /tmp/mlopsartifacts:/tmp/mlopsartifacts
      - /tmp/store:/tmp/store
      - /tmp/mlreports:/tmp/mlreports
      - /Users/jakob/.secrets/mlops-credit-risk-secret.json:/secrets/mlops-credit-risk-secret.json
    networks:
      - backend

  prediction_service:
    container_name: prediction_service
    image: prediction_service
    build:
      context: monitoring/prediction_service
      dockerfile: Dockerfile
    depends_on:
      - evidently_service
      - db
    environment:
      EVIDENTLY_SERVICE: "http://evidently_service.:8877"
      MLFLOW_TRACKING_URI: "http://localhost:${MLFLOW_PORT}"
      # GOOGLE_APPLICATION_CREDENTIALS: /secrets/mlops-credit-risk-secret.json
    # volumes:
    #   - /Users/jakob/.secrets/mlops-credit-risk-secret.json:/secrets/mlops-credit-risk-secret.json
    ports:
      - "9696:9696"
    networks:
      - backend
      - frontend

  evidently_service:
    container_name: evidently_service
    image: evidently_service
    build:
      context: monitoring/evidently_service
      dockerfile: Dockerfile
    depends_on:
      - grafana
    volumes:
      - ./monitoring/evidently_service/datasets:/app/datasets
      - ./monitoring/evidently_service/config.yaml:/app/config.yaml
    ports:
      - "8877:8877"
    networks:
      - backend
      - frontend

  prometheus:
    container_name: prometheus
    image: prom/prometheus
    depends_on:
      - db
    volumes:
      - ./monitoring/evidently_service/config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    ports:
      - "9091:9090"
    networks:
      - backend
    restart: always

  grafana:
    container_name: grafana
    image: grafana/grafana
    user: "472"
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/evidently_service/config/grafana_datasources.yml:/etc/grafana/provisioning/datasources/grafana_datasources.yml:ro
      - ./monitoring/evidently_service/config/grafana_dashboards.yml:/etc/grafana/provisioning/dashboards/grafana_dashboards.yml:ro
      - ./monitoring/evidently_service/dashboards:/opt/grafana/dashboards
      - ./monitoring/grafana_data:/var/lib/grafana
    networks:
      - backend
      - frontend
    restart: always
  
  reporting:
    restart: always
    container_name: reporting_agent
    image: reporting_agent
    build:
      context: ./monitoring
      dockerfile: Dockerfile
    depends_on:
      - prefect_server
      - mlflow_server
      - db
    environment:
      # GOOGLE_APPLICATION_CREDENTIALS: /secrets/mlops-credit-risk-secret.json
      SQLALCHEMY_DATABASE_URI: "postgresql://${PG_USER}:${PG_PASSWORD}@db:${PG_PORT}/${PG_DATABASE}"
    # volumes:
    #   - /Users/jakob/.secrets/mlops-credit-risk-secret.json:/secrets/mlops-credit-risk-secret.json
    networks:
      - backend

volumes:
    prometheus_data:
    grafana_data:
    db_data:
    minio_data:

networks:
  frontend:
        driver: bridge
  backend:
        driver: bridge
